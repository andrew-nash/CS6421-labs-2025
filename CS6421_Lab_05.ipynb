{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 5: Anomaly Detection Autoencoders\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/andrew-nash/CS6421-labs-2025/blob/main/CS6421_Lab_05.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "Source: https://www.tensorflow.org/tutorials/generative/autoencoder#third_example_anomaly_detection\n",
        "\n",
        "In this lab we will look at modelling non-image data with autoencoders - specifically, we will be taking ECG (Electrocardiogram) signals, and uing an autoencoder to identify potential abnormalities in the signals"
      ],
      "metadata": {
        "id": "jaVLE7rzCgmK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "sOC-7TY7DqsW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Data\n",
        "\n",
        "We will be using the [ECG5000 Dataset](http://www.timeseriesclassification.com/description.php?Dataset=ECG5000). This countains 5000 ECG signals each of 140 samples. Each signal has a label indicating whether it contains normal or abnormal behaviour."
      ],
      "metadata": {
        "id": "r1sxfCuiDJnA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataframe = pd.read_csv('http://storage.googleapis.com/download.tensorflow.org/data/ecg.csv', header=None)\n",
        "raw_data = dataframe.values\n",
        "dataframe.head()"
      ],
      "metadata": {
        "id": "rQ-YL3XmCgM-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-kBp5s9CULQ"
      },
      "outputs": [],
      "source": [
        "# The last element contains the labels\n",
        "labels = raw_data[:, -1]\n",
        "\n",
        "# The other data points are the electrocadriogram data\n",
        "data = raw_data[:, 0:-1]\n",
        "\n",
        "train_data, valid_data, train_labels, valid_labels = train_test_split(\n",
        "    data, labels, test_size=0.2, random_state=21\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels"
      ],
      "metadata": {
        "id": "4u8ZJdGpEJGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The only pre-processing needed will be to scalem the data to be between 0 and 1"
      ],
      "metadata": {
        "id": "hbH69qWtD0Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "min_val = tf.reduce_min(train_data)\n",
        "max_val = tf.reduce_max(train_data)\n",
        "\n",
        "train_data = (train_data - min_val) / (max_val - min_val)\n",
        "valid_data = (valid_data - min_val) / (max_val - min_val)\n",
        "\n",
        "train_data = tf.cast(train_data, tf.float32)\n",
        "valid_data = tf.cast(valid_data, tf.float32)"
      ],
      "metadata": {
        "id": "HFcrJEJSDxQb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# convert the 1s and 0s to True/False\n",
        "train_labels = train_labels.astype(bool)\n",
        "valid_labels = valid_labels.astype(bool)\n",
        "\n",
        "#    train_data[ [True,False,True,False,False,...] ] will return the values of train_data\n",
        "#                                                    at indices that correspond to True\n",
        "normal_train_data = train_data[train_labels]\n",
        "normal_valid_data = valid_data[valid_labels]\n",
        "\n",
        "anomalous_train_data = train_data[~train_labels]\n",
        "anomalous_valid_data = valid_data[~valid_labels]"
      ],
      "metadata": {
        "id": "OfLIcXsJD6x2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(140), normal_train_data[0])\n",
        "plt.title(\"A Normal ECG\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BYvmbqLTD8TW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.grid()\n",
        "plt.plot(np.arange(140), anomalous_train_data[0])\n",
        "plt.title(\"An Anomalous ECG\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-PRDW9fQEhUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelling\n",
        "\n",
        "We are going to approach this problem in what might perhaps be a surprising manner.\n",
        "\n",
        "We will **not** fit a regression/classification deep model onto the data directly.\n",
        "\n",
        "Instead, we will fit an autoecnoder to the signals and attempt to reconstruct its inputs.\n",
        "\n",
        "This can be considered to be a model that tries as best as possible to learn a *\"normal\"* model of ECG signal behaviour. Then, we will compare the *actual* signals, to their reconstruction - our assumption is that deviations from the reconstructions are caused by anomalous behavior.\n",
        "\n",
        "<strong>What is the benefit of this over a more traditional approach?</strong>"
      ],
      "metadata": {
        "id": "c5ruJt_BEnoz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating the reconstruction autoencoder model"
      ],
      "metadata": {
        "id": "JJqh7WbwIGGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir tboard"
      ],
      "metadata": {
        "id": "CMmKTEj-HLAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnomalyDetector(tf.keras.models.Model):\n",
        "  def __init__(self):\n",
        "    super(AnomalyDetector, self).__init__()\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(8, activation=\"relu\")])\n",
        "\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "      tf.keras.layers.Dense(140, activation=\"sigmoid\")])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded\n",
        "\n",
        "autoencoder = AnomalyDetector()\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(f\"./tboard/dense_basic\", histogram_freq=1)\n",
        "autoencoder.fit(normal_train_data, normal_train_data,\n",
        "          epochs=20,\n",
        "          batch_size=512,\n",
        "          validation_data=(valid_data, valid_data),\n",
        "          shuffle=True, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "tx6InqEgG3sc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us now look at the reconstruction of a normal (non-anaomalous) ECG signal"
      ],
      "metadata": {
        "id": "_BlFqJC9HwmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data = autoencoder.encoder(normal_valid_data).numpy()\n",
        "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
        "\n",
        "plt.plot(normal_valid_data[0], 'b')\n",
        "plt.plot(decoded_data[0], 'r')\n",
        "plt.fill_between(np.arange(140), decoded_data[0], normal_valid_data[0], color='lightcoral')\n",
        "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Clj6FU2NHawN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "... compared to an anomalous signal:"
      ],
      "metadata": {
        "id": "1MSf2S2aH-nX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_data = autoencoder.encoder(anomalous_valid_data).numpy()\n",
        "decoded_data = autoencoder.decoder(encoded_data).numpy()\n",
        "\n",
        "plt.plot(anomalous_valid_data[0], 'b')\n",
        "plt.plot(decoded_data[0], 'r')\n",
        "plt.fill_between(np.arange(140), decoded_data[0], anomalous_valid_data[0], color='lightcoral')\n",
        "plt.legend(labels=[\"Input\", \"Reconstruction\", \"Error\"])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "krbKEVP9HvZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Detecting the anomalies\n",
        "\n",
        "Detect anomalies by calculating whether the reconstruction loss is greater than a fixed threshold. In this tutorial, you will calculate the mean average error for normal examples from the training set, then classify future examples as anomalous if the reconstruction error is higher than one standard deviation from the training set.\n",
        "\n",
        "Plot the reconstruction error on normal ECGs from the training set"
      ],
      "metadata": {
        "id": "RPOAKZHIICIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructions = autoencoder.predict(normal_train_data)\n",
        "train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n",
        "\n",
        "plt.hist(train_loss[None,:], bins=50)\n",
        "plt.xlabel(\"Train loss\")\n",
        "plt.ylabel(\"No of examples\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gF8yCEuHIEK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = np.mean(train_loss) + np.std(train_loss)\n",
        "print(\"Threshold: \", threshold)"
      ],
      "metadata": {
        "id": "8CcuWT06IQrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are many heuristics for choosing this threshold.\n",
        "\n",
        "Next, look at the distribution of reconstruction errors for the validation data"
      ],
      "metadata": {
        "id": "C7Zh2zQmIzKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructions = autoencoder.predict(anomalous_valid_data)\n",
        "valid_loss = tf.keras.losses.mae(reconstructions, anomalous_valid_data)\n",
        "\n",
        "plt.hist(valid_loss[None, :], bins=50)\n",
        "plt.xlabel(\"Validation loss\")\n",
        "plt.ylabel(\"No of examples\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Tk1HRno8ITFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, data, threshold):\n",
        "  reconstructions = model(data)\n",
        "  loss = tf.keras.losses.mae(reconstructions, data)\n",
        "  return tf.math.less(loss, threshold)\n",
        "\n",
        "def print_stats(predictions, labels):\n",
        "  print(\"Accuracy = {}\".format(accuracy_score(labels, predictions)))\n",
        "  print(\"Precision = {}\".format(precision_score(labels, predictions)))\n",
        "  print(\"Recall = {}\".format(recall_score(labels, predictions)))"
      ],
      "metadata": {
        "id": "THvIgc4IIZRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = predict(autoencoder, valid_data, threshold)\n",
        "print_stats(preds, valid_labels)"
      ],
      "metadata": {
        "id": "xjyZFVsoIbpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Auto-encoder Variants - Contractive Autoncoder\n",
        "\n",
        "Already, we can see that the above auto-encoder is an under-complete autoncoder.\n",
        "\n",
        "We can extend it to be a contractive autoencoder by adding a regularizing loss term:\n",
        "\n",
        "\\begin{equation}\n",
        "L = L(x - \\hat{x}) + \\lambda\\sum_i ||\\nabla_x h(x)||\n",
        "\\end{equation}\n",
        "\n",
        "Where $h(x)$ isthe bottleneck"
      ],
      "metadata": {
        "id": "GAgVn0YHPp0H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contractive Auto-Encoder"
      ],
      "metadata": {
        "id": "gWlUp-gCU31B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ContractiveAutoencoder(tf.keras.Model):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(32,input_shape=(140,), activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(8, activation=\"relu\")\n",
        "        ])\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(32, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(140, activation=\"sigmoid\")\n",
        "        ])\n",
        "    def call(self, x):\n",
        "        encoded = self.encoder(x)\n",
        "        decoded = self.decoder(encoded)\n",
        "        self.add_loss(self.contractive_loss(x))\n",
        "\n",
        "        return decoded\n",
        "\n",
        "    def contractive_loss(self, x):\n",
        "        with tf.GradientTape() as tape:\n",
        "            tape.watch(x)\n",
        "            encoded = self.encoder(x)\n",
        "        jacobian = tape.batch_jacobian(encoded, x)\n",
        "        contractive_loss = tf.reduce_sum(tf.square(jacobian), axis=(1,2))\n",
        "        return 1e-4 * contractive_loss\n",
        "\n",
        "cae = ContractiveAutoencoder()\n",
        "cae.compile(optimizer='adam', loss='mse')\n",
        "\n",
        "cae.fit(normal_train_data, normal_train_data,\n",
        "          epochs=20,\n",
        "          batch_size=512,\n",
        "          validation_data=(valid_data, valid_data),\n",
        "          shuffle=True, callbacks=[tensorboard_callback])"
      ],
      "metadata": {
        "id": "fGA0_lAxgmaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reconstructions = autoencoder.predict(normal_train_data)\n",
        "train_loss = tf.keras.losses.mae(reconstructions, normal_train_data)\n",
        "\n",
        "reconstructions = autoencoder.predict(anomalous_valid_data)\n",
        "valid_loss = tf.keras.losses.mae(reconstructions, anomalous_valid_data)\n",
        "\n",
        "preds = predict(cae, valid_data, threshold)\n",
        "print_stats(preds, valid_labels)"
      ],
      "metadata": {
        "id": "VC_-PXXDnUaq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}