{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHI8oLVYcolR"
      },
      "source": [
        "# Lab 4 - A Deeper Dive on CNNs\n",
        "  <a target=\"_blank\" href=\"https://colab.research.google.com/github/andrew-nash/CS6421-labs-2025/blob/main/CS6421_Lab_04.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MjbPNQP8ddI6"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcE3zaaZ5F8o"
      },
      "source": [
        "# Data Loading And Cleaning\n",
        "\n",
        "For this lab, we will continue to use the MNIST dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mjoxz44ScZ4v"
      },
      "outputs": [],
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# data normalizing\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sx3TrRA6dfdd"
      },
      "outputs": [],
      "source": [
        "print(\"Train shape\", x_train.shape)\n",
        "print(\"Test shape\",  x_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GePMzVY2dhXk"
      },
      "outputs": [],
      "source": [
        "plt.imshow(x_train[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUlvrNWipRJI"
      },
      "outputs": [],
      "source": [
        "plt.imshow(x_test[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwCJZ8S0go6R"
      },
      "source": [
        "Currently, `x_train` and `x_test` are arrays of square 28x28 greyscale images. For compatibility with CNN models, as per the last lab, we will reshape the dataset to add a colour channel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cAflGgQvgwSv"
      },
      "outputs": [],
      "source": [
        "x_train_clean = x_train.reshape(-1,28,28,1)\n",
        "x_test_clean = x_test.reshape(-1,28,28,1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FDsBHBMiy8Y"
      },
      "source": [
        "Now lets consider our output labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0S_rGjstiiBW"
      },
      "outputs": [],
      "source": [
        "y_train.shape, y_test.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SzmYNEZi4Nr"
      },
      "source": [
        "\n",
        "\n",
        "### One-hot Encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBytsRXsi3pz"
      },
      "outputs": [],
      "source": [
        "y_train_clean = tf.one_hot(indices=y_train, depth=10)\n",
        "y_test_clean = tf.one_hot(indices=y_test, depth=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZqJBH2qmfm7"
      },
      "source": [
        "It is important to note that if your labels do not consist of the integers $0,1,2,3,4,\\dots$ additional processing will be required to produce the one-hot vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rdm03-Gsza_C"
      },
      "source": [
        "### Softmax Activation\n",
        "\n",
        "We previously discussed the softmax activation function, that maps a set of arbritary activations to a probability distribution.\n",
        "\n",
        "The formula for this is\n",
        "\n",
        "\\begin{equation}\n",
        "  softmax(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^Ne^{x_j}}\n",
        "\\end{equation}\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def softmax(x):\n",
        "  return tf.exp(x) / tf.reduce_sum(tf.exp(x))"
      ],
      "metadata": {
        "id": "bFi0PTrTOuXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cross-Entropy Loss\n",
        "Cross entropy loss rewards low-entropy predicted probability distributions, which have high confidence in the predicted class.\n",
        "\n",
        "\\begin{equation}\n",
        "  CE(y, p) = -\\sum_{i=1}^{N}y_i\\log(p_i)\n",
        "\\end{equation}\n",
        "\n",
        "Where $y$ is the one-hot vector encoding the true class, $p$ is the predicted probability distribution over the classes."
      ],
      "metadata": {
        "id": "xFw_rHQQOtgx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def cross_entropy(y, p):\n",
        "  # Shape of y and p are each (BATCH SIZE, 10)\n",
        "  # if we don't use the tf.reduce_mean, and axis=1, this will compute the\n",
        "  # sum of the loss of each sample in the batch\n",
        "  # By including the reduce_mean, we are getting the avergae loss over the batch\n",
        "  return tf.reduce_mean(-tf.reduce_sum(y * tf.math.log(p),axis=1))"
      ],
      "metadata": {
        "id": "5DZmBC-nQRZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.reduce_sum(np.ones((16,10)), axis=1)"
      ],
      "metadata": {
        "id": "3-SFGvZObG4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDmFSeI3Cqi5"
      },
      "outputs": [],
      "source": [
        "!pip install -U keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyo0RYS8H7nn"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG4ghsYRCnRE"
      },
      "outputs": [],
      "source": [
        "import keras_tuner as kt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTe19F9L0_tH"
      },
      "source": [
        "# Visualizing CNN Filters\n",
        "\n",
        "In this lab, we will create a simple CNN model similarly to the last lab, but this time we will try to viualize the filters that are being learned to better understand the model's behaviour."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Visualize the feature maps for the first image in the Test Dataset"
      ],
      "metadata": {
        "id": "rdRkqZPaWOdT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Input(shape=(28,28,1)))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(4,4), strides=(1,1), activation=\"elu\"))\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(10, activation=softmax))\n",
        "\n",
        "\n",
        "model.compile(loss=cross_entropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train_clean, y_train_clean, validation_split=0.2, epochs=5, batch_size=16)"
      ],
      "metadata": {
        "id": "Xrq4S34JWRnj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filters, biases = model.layers[0].get_weights()\n",
        "\n",
        "N_FILTERS = filters.shape[-1]\n",
        "\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)\n",
        "\n",
        "for i in range(N_FILTERS):\n",
        "  plt.subplot(2,4,i+1)\n",
        "  plt.imshow(filters[:,:,0,i], cmap='gray')\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "rQeNqnaWYIgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Keras allows us to extract feature maps from any part of our model as follows:"
      ],
      "metadata": {
        "id": "k-LDZgJsdQjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_subsegment = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
        "\n",
        "feature_maps = model_subsegment(x_test[0].reshape(1,28,28,1)).numpy()\n",
        "N_FILTERS = feature_maps.shape[-1]\n",
        "f_min, f_max = feature_maps.min(), feature_maps.max()\n",
        "features = (feature_maps - f_min) / (f_max - f_min)\n",
        "\n",
        "\n",
        "for i in range(N_FILTERS):\n",
        "  plt.subplot(2,4,i+1)\n",
        "  plt.imshow(features[:,:,:,i].reshape(25,25), cmap='gray')\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "5qzdjcfZXLAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And after maxpool, ..."
      ],
      "metadata": {
        "id": "yv7b2zhdcZdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_subsegment = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
        "\n",
        "feature_maps = model_subsegment(x_test[0].reshape(1,28,28,1)).numpy()\n",
        "N_FILTERS = feature_maps.shape[-1]\n",
        "f_min, f_max = feature_maps.min(), feature_maps.max()\n",
        "features = (feature_maps - f_min) / (f_max - f_min)\n",
        "\n",
        "\n",
        "for i in range(N_FILTERS):\n",
        "  plt.subplot(2,4,i+1)\n",
        "  plt.imshow(features[:,:,:,i].reshape(12,12), cmap='gray')\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "34x75SUacW6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_subsegment = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
        "\n",
        "feature_maps = model_subsegment(x_test[3].reshape(1,28,28,1)).numpy()\n",
        "N_FILTERS = feature_maps.shape[-1]\n",
        "f_min, f_max = feature_maps.min(), feature_maps.max()\n",
        "features = (feature_maps - f_min) / (f_max - f_min)\n",
        "\n",
        "\n",
        "for i in range(N_FILTERS):\n",
        "  plt.subplot(2,4,i+1)\n",
        "  plt.imshow(features[:,:,:,i].reshape(25,25), cmap='gray')\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "B930i4IeXI7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using a model with regularizatoin"
      ],
      "metadata": {
        "id": "IAfSnUlUT7Qb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.Input(shape=(28,28,1)))\n",
        "\n",
        "model.add(tf.keras.layers.Conv2D(filters=8, kernel_size=(4,4), strides=(1,1), activation=\"elu\", kernel_regularizer=tf.keras.regularizers.L1L2(0.01)))\n",
        "model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n",
        "\n",
        "model.add(tf.keras.layers.BatchNormalization())\n",
        "\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(10, activation=softmax))\n",
        "\n",
        "\n",
        "model.compile(loss=cross_entropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(x_train_clean, y_train_clean, validation_split=0.2, epochs=5, batch_size=16)"
      ],
      "metadata": {
        "id": "B4oKi_iaZkgz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filters, biases = model.layers[0].get_weights()\n",
        "\n",
        "N_FILTERS = filters.shape[-1]\n",
        "\n",
        "f_min, f_max = filters.min(), filters.max()\n",
        "filters = (filters - f_min) / (f_max - f_min)\n",
        "\n",
        "for i in range(N_FILTERS):\n",
        "  plt.subplot(2,4,i+1)\n",
        "  plt.imshow(filters[:,:,0,i], cmap='gray')\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "w1Gww9nkbk31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_subsegment = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
        "\n",
        "feature_maps = model_subsegment(x_test[0].reshape(1,28,28,1)).numpy()\n",
        "N_FILTERS = feature_maps.shape[-1]\n",
        "f_min, f_max = feature_maps.min(), feature_maps.max()\n",
        "features = (feature_maps - f_min) / (f_max - f_min)\n",
        "\n",
        "\n",
        "for i in range(N_FILTERS):\n",
        "  plt.subplot(2,4,i+1)\n",
        "  plt.imshow(features[:,:,:,i].reshape(25,25), cmap='gray')\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "ja0_mZr6cA9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After the maxpool, ..."
      ],
      "metadata": {
        "id": "aOUQmnE4cQkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_subsegment = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
        "\n",
        "feature_maps = model_subsegment(x_test[0].reshape(1,28,28,1)).numpy()\n",
        "N_FILTERS = feature_maps.shape[-1]\n",
        "f_min, f_max = feature_maps.min(), feature_maps.max()\n",
        "features = (feature_maps - f_min) / (f_max - f_min)\n",
        "\n",
        "\n",
        "for i in range(N_FILTERS):\n",
        "  plt.subplot(2,4,i+1)\n",
        "  plt.imshow(features[:,:,:,i].reshape(12,12), cmap='gray')\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "YB_rXnPscR7H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_subsegment = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[0].output)\n",
        "\n",
        "feature_maps = model_subsegment(x_test[3].reshape(1,28,28,1)).numpy()\n",
        "N_FILTERS = feature_maps.shape[-1]\n",
        "f_min, f_max = feature_maps.min(), feature_maps.max()\n",
        "features = (feature_maps - f_min) / (f_max - f_min)\n",
        "\n",
        "\n",
        "for i in range(N_FILTERS):\n",
        "  plt.subplot(2,4,i+1)\n",
        "  plt.imshow(features[:,:,:,i].reshape(25,25), cmap='gray')\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "gsDqg6Rvdgd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_subsegment = tf.keras.models.Model(inputs=model.inputs, outputs=model.layers[1].output)\n",
        "\n",
        "feature_maps = model_subsegment(x_test[3].reshape(1,28,28,1)).numpy()\n",
        "N_FILTERS = feature_maps.shape[-1]\n",
        "f_min, f_max = feature_maps.min(), feature_maps.max()\n",
        "features = (feature_maps - f_min) / (f_max - f_min)\n",
        "\n",
        "\n",
        "for i in range(N_FILTERS):\n",
        "  plt.subplot(2,4,i+1)\n",
        "  plt.imshow(features[:,:,:,i].reshape(12,12), cmap='gray')\n",
        "  plt.axis('off')"
      ],
      "metadata": {
        "id": "f7enNYU5doQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transfer Learning\n",
        "\n",
        "In your early lectures, you have seen the concepts of Transfer Learning.\n",
        "\n",
        "The following is a simple example of using a pre-trained image classifcation model with some light transfer learning."
      ],
      "metadata": {
        "id": "Ke5C2sAncu4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Typically, we only use trasnfer learning on more complex data - we will upscale\n",
        "# the MNIST data to simulate this, and include colour channels\n",
        "x_train_modified = tf.image.grayscale_to_rgb(tf.image.resize(x_train_clean, (56,56)))\n",
        "x_test_modified = tf.image.grayscale_to_rgb(tf.image.resize(x_test_clean, (56,56)))"
      ],
      "metadata": {
        "id": "tIPw9HTeh_jD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model = tf.keras.applications.ConvNeXtTiny(\n",
        "    include_top=False,\n",
        "    weights=\"imagenet\",\n",
        "    input_shape=(56,56,3)\n",
        ")"
      ],
      "metadata": {
        "id": "S4shcukucwGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model.summary()"
      ],
      "metadata": {
        "id": "j5U1ksNpfGRe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We don't want to undo the extensive training this model has undergone, so we shall freeze the weights and biases"
      ],
      "metadata": {
        "id": "sG4SOyoCgaqx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pre_trained_model.trainable = False"
      ],
      "metadata": {
        "id": "x2deSv5XgUNw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our input shape is satisfactory for our re-processed data, but we need to add an appropiate final Dense layer to get the correct output shape.\n",
        "\n",
        "This uses the Keras functional API"
      ],
      "metadata": {
        "id": "6pAbwQMsglRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_layer = tf.keras.layers.Dense(10, activation=softmax)(pre_trained_model.output)"
      ],
      "metadata": {
        "id": "-Z9M5lE6gjK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add this to the model"
      ],
      "metadata": {
        "id": "izNMaE_wgyiD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_model = tf.keras.models.Model(pre_trained_model.input, final_layer)"
      ],
      "metadata": {
        "id": "tgj617-3hYbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "our_model.summary()"
      ],
      "metadata": {
        "id": "u7HEA7exhe2_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the only weights and biases that will be trained are those in our final Dense layer."
      ],
      "metadata": {
        "id": "Cw7BIDGKhhub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_model.compile(loss=cross_entropy,\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "our_model.fit(x_train_modified, y_train_clean, validation_split=0.2, epochs=5, batch_size=16)"
      ],
      "metadata": {
        "id": "LY8WtMHZhggN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you see, even with a 'Tiny' model, this can take a cponsiderable amount of time to train"
      ],
      "metadata": {
        "id": "Zkp_kqu9ihGx"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}