{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Lab 07 - Trends in Deep Learning\n",
        "\n",
        "<a target=\"_blank\" href=\"https://colab.research.google.com/github/andrew-nash/CS6421-labs-2025/blob/main/CS6421_Lab_07.ipynb\">\n",
        "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
        "</a>\n",
        "\n",
        "\n",
        "For our final lab, I will provide pointers to some useful and important libraries that will be relevant to working in the area of deep learning going forwards.\n",
        "\n",
        "I will also provide some pointers to some interesting current topics in DL, and pointers towards reference impementations.\n",
        "\n"
      ],
      "metadata": {
        "id": "Igrb1rNRgeOQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning Libraries in Practice - Pytorch\n",
        "\n",
        "In these labs, we have worked exclusively in TensorFlow.\n",
        "\n",
        "\n",
        "Taken from: https://github.com/pytorch/examples/blob/main/mnist/main.py\n",
        "\n",
        "You will find many people, particularly in academic research, work in and publish models in Pytorch"
      ],
      "metadata": {
        "id": "RziW7yLGg8j7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.optim.lr_scheduler import StepLR"
      ],
      "metadata": {
        "id": "9Y4MYIYmhbkq"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        ])\n",
        "\n",
        "dataset1 = datasets.MNIST('./data', train=True, download=True,\n",
        "                    transform=transform)\n",
        "dataset2 = datasets.MNIST('./data', train=False,\n",
        "                    transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset1)\n",
        "test_loader = torch.utils.data.DataLoader(dataset2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqEIa64ShorN",
        "outputId": "840d8085-ed23-4444-a40b-817969417478"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 22.2MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 599kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.85MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 5.22MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
        "        self.dropout1 = nn.Dropout(0.25)\n",
        "        self.dropout2 = nn.Dropout(0.5)\n",
        "        self.fc1 = nn.Linear(9216, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = F.relu(x)\n",
        "        x = F.max_pool2d(x, 2)\n",
        "        x = self.dropout1(x)\n",
        "        x = torch.flatten(x, 1)\n",
        "        x = self.fc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.fc2(x)\n",
        "        output = F.log_softmax(x, dim=1)\n",
        "        return output\n"
      ],
      "metadata": {
        "id": "-xAioQDwg71G"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WgUcmAimgcv8"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cpu\")\n",
        "\n",
        "model = Net().to(device)\n",
        "optimizer = optim.Adadelta(model.parameters(), lr=0.001)\n",
        "loss_f = nn.CrossEntropyLoss()\n",
        "\n",
        "epochs = 10\n",
        "\n",
        "\n",
        "\n",
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))\n",
        "\n",
        "\n",
        "scheduler = StepLR(optimizer, step_size=1)\n",
        "log_interval = 1000\n",
        "for epoch in range(1, epochs + 1):\n",
        "    # here, train() only acts as in indicatoin that we are training the model\n",
        "    # we need to be more verbose in applying backpropogation than TF\n",
        "    model.train()\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(data)\n",
        "        loss = loss_f(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % log_interval == 0:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "    test(model, device, test_loader)\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), \"mnist_cnn.pt\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key differences between TensorFlow and Keras are that:\n",
        "\n",
        "1. Many consider Pytorch is easier for prototyping - pytorch syntax is closer to standard Python syntax than TensorFlow, and computation graphs are dynamically computed (compared to TensorFlow's static computation graphs)\n",
        "2. Customizing, deploying, scaling and vizualising models is more effective in TensorFlow\n",
        "\n",
        "More information is available on their website: https://pytorch.org/tutorials/ .\n",
        "\n",
        "It is sometimes possible to convert models that were trained in Pytorch to be usable in TensorFlow and vice-versa (depending on the specific operations and layers used), using standardized models such as ONNX - https://github.com/onnx/onnx .\n",
        "\n"
      ],
      "metadata": {
        "id": "lwjDntxZj8ID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accelerating Operations in Python on GPU and TPU - JAX\n",
        "\n",
        "As the prevelance of Deep Learning has grown, and the size and complexity of models in academia and industry, the importance of efficient utilization of GPU and TPU hardware acceleration of linear algebra operations has grown accordingly.\n",
        "\n",
        "As we have seen in these labs, the basis of most Deep Learning with Python, and scientific computing using Python in general, relies on NumPy. While more efficient than base Python, Numpy operations only utilize CPU resources.\n",
        "\n",
        "In 2018, Google released JAX which in effect is an implementation of the Numpy API (and some of SciPy's API) with:\n",
        "\n",
        "1. Operation vectorization\n",
        "2. Auto-differentiation\n",
        "3. Parrallel computation\n",
        "\n",
        "While fully utilizing GPU and TPU accelaration, with the capability of a Just-in-time (JIT) compiler.\n",
        "\n",
        "\n",
        "Since its intorduction, JAX has grown in popularity and will likely to become an increasingly important component of scientific computing in Python. I would expect that at some point in the future, data processing and defining custom operations in TensorFlow, Keras and other libraries will rely on working in JAX.\n",
        "\n",
        "\n",
        "Example taken from: https://docs.jax.dev/en/latest/quickstart.html\n",
        "\n",
        "If you are interested, there are good tutorials available on JAX's own website.\n"
      ],
      "metadata": {
        "id": "RxnBZ3TxlF3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp"
      ],
      "metadata": {
        "id": "D3Fm1ukNoBnD"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def selu(x, alpha=1.67, lmbda=1.05):\n",
        "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "\n",
        "x = jnp.arange(5.0)\n",
        "print(selu(x))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ei8LJJHCn_KB",
        "outputId": "936bbced-de09-4f2a-954e-a14cfd639485"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.        1.05      2.1       3.1499999 4.2      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just0in-time compilation can be enabled simply:"
      ],
      "metadata": {
        "id": "coySI78UojOq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import jit\n",
        "selu_jit = jit(selu)\n",
        "\n",
        "selu_jit(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0F8JiAXoKlo",
        "outputId": "ef6c26c2-8e7a-4ce5-c4d1-af14b470fd61"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([0.       , 1.05     , 2.1      , 3.1499999, 4.2      ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Auto-differeentiation is built-in, and slightly more straightforward than in TensorFlow"
      ],
      "metadata": {
        "id": "MuobbslOoqRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import grad\n",
        "\n",
        "def sum_logistic(x):\n",
        "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
        "\n",
        "x_small = jnp.arange(3.)\n",
        "derivative_fn = grad(sum_logistic)\n",
        "print(derivative_fn(x_small))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mr_mFF-Aonbe",
        "outputId": "6f0ac9de-b6cf-4111-e5c7-1c68d0f3c7e7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.25       0.19661197 0.10499357]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "JAX is not a silver bullet, however - for small datasets, the overhead may mean that JAX runs slower than straightforward NumPy operations on CPU.\n",
        "\n",
        "There also some noticeable limitations to JAX operations compared to NumPy (https://docs.jax.dev/en/latest/notebooks/Common_Gotchas_in_JAX.html):\n",
        "\n",
        "1. Global variables are prohibited in JAX functions - functions must all be \"pure\", where all inputs are passed as arguments, and all the results are returned as the function's output - side effects, even those of print statements inside the functions can behave unexpectedly. Read the section on \"Pure functions\" above for more deails.\n",
        "2. In-place updates, i.e. updating elements or ranges of elements by index are performed functionally, since JAX arrays are immutable `x.at[idx].set(y)`"
      ],
      "metadata": {
        "id": "zLxLRz83pXdf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Current Topics in Deep Learning\n",
        "\n",
        "\n",
        "The following is not exhuastive, but is a selection of some interesting important topics in current Deep LEarning research & common practice, that involve implementational details outside the scope of these labs. I have provided links to documentation and tutorials on each.\n"
      ],
      "metadata": {
        "id": "NRO1pGzntfEw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Deployment\n",
        "\n",
        "While the techniques you have seen so far are sufficient for prototyping, when developing models in industry, more steps are involved.\n",
        "\n",
        "When using deep models for practical purposes, companies will develop pipelines for data processing, model definiton, hyper-parameter tuning and deployment. This broad process falls under the title of MLOps.\n",
        "\n",
        "There are many MLOps libraries in use, but TFX https://www.tensorflow.org/tfx is a well-documented solution for this that is closely integrated with the models you have seen so far.\n",
        "\n",
        "TensorFlow also provide some very specific case studies that I would encourage you to read (https://www.tensorflow.org/about/case-studies?filter=TFX) that show exactly how this process is applied in the real world."
      ],
      "metadata": {
        "id": "KWX5kA9KuKro"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DpYb1bwW6T6Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Edge AI, Model Optimization and TinyML\n",
        "\n",
        "With the increasing size of models being applied in practice, there is much focus on methods to compress models for the most efficient and low-cost, low-latency inference possible - particular on mobile devices.\n",
        "\n",
        "Common techniques for model optimization are:\n",
        "\n",
        "1. Quantization, where weight and bias values are converted to a more compact (or efficiently utilizable) data type, either post- or during training\n",
        "2. Pruning, where weight and bias values that contribute little to effetive classification are removed\n",
        "3. ADVACNED - model distillation, where large models (teachers) are trained to high performance on a particular datasest and then smaller models (students) are trained against the larger models. https://keras.io/examples/vision/knowledge_distillation/\n",
        "\n",
        "TensorFlow features a full toolkit for model optimizatoin: https://www.tensorflow.org/model_optimization\n",
        "\n",
        "\n",
        "LiteRT (previously TFLite) https://ai.google.dev/edge/litert provides excellent resources for compressing deep models, and provides efficient runtimes and SDKs for various platforms and languages.\n",
        "\n",
        "This includes LiteRT for Microcontrollers https://ai.google.dev/edge/litert/microcontrollers/overview a C++ library that enables models to be executed on microcontrollers.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ViFubpBAt7aj"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RcaxwlaW6SfJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Responsible and Explainable AI, Fiarness & Privacy\n",
        "\n",
        "New EU legistlation, https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai, has placed massive focus on AI givernanace, transperancy, safety & security and fairness.\n",
        "\n",
        "TensorFlow provides a basic set of guides to introduce some of the important concepts in this area. https://www.tensorflow.org/responsible_ai\n",
        "\n",
        "Of these, Federated Learning in particular for privacy-preserving applications has become a topic of much research lately."
      ],
      "metadata": {
        "id": "nJcb1aEZt5HI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B41Ts9f46TBP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}